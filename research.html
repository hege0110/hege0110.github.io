---
layout: page
title: Research
permalink: /research/
---

<h2>
AEROSPACE CONTROLS LAB
</h2>
<h1>
API Design and Implementation for Augmented Reality and Robotic Prototyping System (<a href="/assets/API_DESIGN_AND_IMPLEMENTATION_FOR_ANUGMENTED_REALITY_AND_ROBOTIC_PROTOTYPING_SYSTEM.pdf">paper</a>)
</h1>

<b>
Introduction
</b>
<p>
Although the decreasing cost of electronic hardware has made unmanned aerial vehicles an affordable platform for developing autonomous algorithms, there are still many challenges facing this area of research including planning, control, perception and learning. When designing software models that tackles these problems, it is necessary to test the model’s performance on live robotic hardware. Performing live tests are a key step in transitioning algorithms from software simulation to real-world systems. These tests are important in understanding how the algorithm performs in different tasks and reacts to various kinds of environmental noise. In general, when running algorithms on physical robotics systems, new challenges that are often unrelated to the primary goals of the investigation present themselves, including issues of hardware failure and noisy environments. When running algorithms on flying robots or quads, these issues are magnified with the added concern of unsafe failure modes. When a ground robot fails, at worst it collides with a wall or stubs someone’s toe. When a quad fails, spinning propellers in the air may potentially cause more damage. With these challenges, the overhead of creating new demonstrations are time consuming obstacles that are often burdensome to researchers.</p>

<h2>
Computational Cognitive Science 
</h2>
<h1>
Validating Annotations in Verb Corner using Mixture Models (<a href="/assets/validating_annotations_in_verb_corner_using_mixture_models.pdf">paper</a>)</h1>

<b>
Abstract
</b>
<p>
VerbCorner is an online linguistics project which works to figure out what verbs mean using semantic structure and further understand how we communicate with one another. Figuring out what verbs mean is a large undertaking and requires many annotations to be made. VerbCorner takes the approach of presenting a short story to go along with a question at the end asking about various types of meaning. With the help of many people, the task of annotating verbs is a lot easier; but then we are left with the task of validating the annotations. To evaluate the quality of the annotations we apply an item- response model described in Hovy’s paper entitled Learning Whom to Trust with MACE[1]. The system that learns in an unsupervised way to do two things. 1) Identify which annotators are trustworthy and 2) predict the correct labels. We implemented a basic version of the model they described using probabilistic programming language Venture. Then we had the model run this model on the crowd sourced annotations on the semantic structure of verbs generated by users on VerbCorner. We tested the model in different conditions in hopes of finding out more about our data. Further refinement of the model, including taking a look at the priors distribution across the answers will have to be reconsidered.
</p>
 

